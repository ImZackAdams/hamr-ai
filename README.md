# HAMR  
**Hierarchical Adaptive Memory Retrieval for Large Language Models**  
_White Paper â€¢ GitHub README Format_

---

## Table of Contents
1. [Abstract](#abstract)
2. [Motivation](#motivation)
3. [Problem Statement](#problem-statement)
4. [Related Work](#related-work)
5. [HAMR Architecture](#hamr-architecture)
6. [Memory Layers](#memory-layers)
7. [Runtime Retrieval Algorithm](#runtime-retrieval-algorithm)
8. [Evaluation Methodology](#evaluation-methodology)
9. [Implementation Roadmap](#implementation-roadmap)
10. [Risks and Mitigations](#risks-and-mitigations)
11. [Call to Action](#call-to-action)

---

## Abstract

LLMs crush bite-sized tasks, then face-plant on long hauls.  
Fixed context is the straight-jacket: keep every token and pay through the nose, or prune history and lobotomize the model.  
RAG patches the hole, but vanilla vector search is flat, frozen, and time-blind.

**HAMR** bolts on a human-style memory stack:

- **Short Term** â€“ raw turns, hot off the wire  
- **Mid Term** â€“ summaries when something actually happens  
- **Long Term** â€“ distilled truth, graphs, core facts  

Every query runs the gauntlet: score chunks on meaning, freshness, and weight, then spit out the leanest prompt per token.  
Your assistant remembers what matters, forgets the noise, and never blows the context budget.

---

## Motivation

- Real assistants live for weeks, not seconds.  
- Token ceilings are still physics; 100 K contexts just move the pain to your wallet.  
- We need first-class memory, not another prompt engineering hack.

---

## Problem Statement

1. **Context Explosion** â€“ Chat history balloons way past the window.  
2. **Static Retrieval** â€“ Cosine similarity â‰  memory; itâ€™s blind to time and priority.  
3. **Lossy Truncation** â€“ NaÃ¯ve trimming nukes tomorrowâ€™s critical detail.  

---

## Related Work

### Conversation Buffer  
**âœ”ï¸** Dead simple, keeps everything  
**âŒ** Alzheimerâ€™s for anything beyond the cap

### Summary Memory (LangChain, Zep)  
**âœ”ï¸** Cheap on tokens  
**âŒ** Summaries hallucinate; fine-grained recall is gone

### Vector Retrieval  
**âœ”ï¸** Pulls arbitrary past chunks  
**âŒ** Ignores recency and hierarchy

### Long Context Transformers (Claude 100 K, Gemini)  
**âœ”ï¸** End-to-end attention  
**âŒ** Expensive, slower, still filter-hungry

### Knowledge Graph Memory (GraphRAG, Graphiti)  
**âœ”ï¸** Structured facts you can query  
**âŒ** Heavy ingest, brittle updates  

**HAMR cherry-picks the wins and ditches the baggage.**

---

## HAMR Architecture

### Memory Flow Pipeline

1. **User Query**  
2. â†’ Embed & tag metadata  
3. â†’ Hit **Memory Store**  
   - **Short Term**: raw turns (TTL â‰ˆ 50)  
   - **Mid Term**: summaries (TTL in days)  
   - **Long Term**: forever facts (no TTL)  
4. â†’ Score chunks: similarity, recency, importance  
5. â†’ Rank + trim to fit the token cap  
6. â†’ **Prompt Assembly Engine**: system + memory + query  
7. â†’ Model answers, rinse, repeat  

---

## Memory Layers

### Short Term Layer
- Raw JSON/plain text turns  
- Evict by age or buffer size

### Mid Term Layer
- Summaries on intent flips, decisions, or timeouts  
- Extract-then-abstract to curb hallucinations

### Long Term Layer
- Graph nodes, user profiles, immutable facts  
- Store in Neo4j, SQLite, or plain JSON  
- Importance: pin it or learn it

---

## Runtime Retrieval Algorithm

1. **Embed** â€“ Create vector for the new turn.  
2. **Score** â€“ `score(c) = Î±Â·sim + Î²Â·recency + Î³Â·importance` â€” tune Î±/Î²/Î³, change the game.  
3. **Rank / Filter** â€“ Keep the top until youâ€™re under budget.  
4. **Assemble** â€“ Fresh raw > summary > lore.  
5. **Generate** â€“ Fire the prompt at the model.  
6. **Update Memory** â€“ Log the turn, roll summaries, cement facts.

---

## Evaluation Methodology

| Metric            | Description                                   |
|-------------------|-----------------------------------------------|
| Recall Accuracy   | Answers that hinge on past context            |
| Token Efficiency  | Tokens burned per useful detail               |
| Coherence Score   | Narrative sanity over time                    |
| Latency           | Wall-clock from query to response             |

### Benchmark Suites

- **Long-Term Chat** â€“ 150-turn synthetic support runs  
- **Narrative QA** â€“ Clip-back questions on early plot hooks  
- **Enterprise FAQ** â€“ Multi-session customer drills  

---

## Implementation Roadmap

| Phase | Deliverable                                          | Timeline |
|-------|------------------------------------------------------|----------|
| 0     | Skeleton demo: FAISS + SQLite layers                 | Week 1   |
| 1     | Bench harness: Jupyter eval scripts                  | Week 3   |
| 2     | OSS drop: repo + REST API                            | Month 2  |
| 3     | Integrations: LangChain, Slack bot, Zep plugin       | Month 3  |
| 4     | Extras: streaming ingest, multi-agent gossip         | Month 6  |

---

## Risks and Mitigations

| Risk                  | Impact                     | Mitigation                                    |
|-----------------------|----------------------------|-----------------------------------------------|
| Summary hallucination | Poisoned long-term memory  | Extractive-first chain, guardrail checks      |
| Retrieval latency     | Sluggish UX                | Embedding cache, ANN search, async summaries  |
| Importance bias       | Pinned data dominates      | Re-weight sweeps, user feedback loops         |

---

## Call to Action

HAMR is an invite to build LLMs with a working brain.

- **ðŸ›   Kick the tires** â€“ run the demo  
- **ðŸ›  File issues** â€“ tweak scoring, add stores  
- **ðŸ’¬  Join us** â€“ letâ€™s kill goldfish memory together  

> **Give language models a real memory.**
